[global]
variable markers   = @
task        = UserTask           ; Job uses user written scripts
backend     = local              ; Send to local batch system
workdir = $CMSSW_BASE/src/TTH/MEAnalysis/gc/work.countFH

[jobs]
wall time = 1:00:00

[UserTask]
executable  = count.sh
dataset splitter = FileBoundarySplitter
files per job = 100
input files = common.sh env.sh
dataset =
 datasets/had_V24_3/TT_TuneCUETP8M1_13TeV-powheg-pythia8.txt
 datasets/had_V24_3/ttHToNonbb_M125_13TeV_powheg_pythia8.txt
 datasets/had_V24_3/ttHTobb_M125_13TeV_powheg_pythia8.txt
 datasets/had_V24_3/QCD_HT300to500_TuneCUETP8M1_13TeV-madgraphMLM-pythia8.txt
 datasets/had_V24_3/QCD_HT500to700_TuneCUETP8M1_13TeV-madgraphMLM-pythia8.txt
 datasets/had_V24_3/QCD_HT700to1000_TuneCUETP8M1_13TeV-madgraphMLM-pythia8.txt
 datasets/had_V24_3/QCD_HT1000to1500_TuneCUETP8M1_13TeV-madgraphMLM-pythia8.txt
 datasets/had_V24_3/QCD_HT1500to2000_TuneCUETP8M1_13TeV-madgraphMLM-pythia8.txt
 datasets/had_V24_3/QCD_HT2000toInf_TuneCUETP8M1_13TeV-madgraphMLM-pythia8.txt

[storage]
scratch space used = 5000
scratch space left = 1000
se output files = count.root
se output pattern = job_@MY_JOBID@_@X@
se path = dir://$HOME/tth/gc/count/${GC_TASK_ID}/${DATASETPATH}/
